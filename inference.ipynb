{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GOOGLE_COLAB = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GOOGLE_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (23.3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: tsfresh 0.18.0 has a non-standard dependency specifier matrixprofile>=1.1.10<2.0.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tsfresh or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install numpy\n",
    "!python -m pip install torch\n",
    "!python -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import warnings\n",
    "import transformers\n",
    "from transformers import logging\n",
    "import torch\n",
    "\n",
    "class Calm_Bot():\n",
    "\n",
    "    def __init__(self, dir_path=\".\", print_info=True, offline=True, model_version=\"V6_8\"):\n",
    "        logging.set_verbosity_error()\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        self.print_info = print_info\n",
    "        self.model_version = model_version\n",
    "        self.offline = offline\n",
    "        self.dir_path = dir_path\n",
    "        self.create_dir_paths()\n",
    "        self.max_length = 1024\n",
    "        self.history = \"\"\n",
    "        self.prompt = \"\"\n",
    "        self.past = None\n",
    "        if self.print_info:\n",
    "            print(\"setting device...\")\n",
    "        self.set_device()\n",
    "        if self.print_info:\n",
    "            print(\"loading tokenizer...\")\n",
    "        self.load_tokenizer()\n",
    "        if self.print_info:\n",
    "            print(\"loading GPT-2 model...\")\n",
    "        self.load_model()\n",
    "\n",
    "    def create_dir_paths(self):\n",
    "        try:\n",
    "            os.makedirs(f\"{self.dir_path}/histories\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def set_device(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "    def load_tokenizer(self):\n",
    "        if self.offline:\n",
    "            self.tokenizer = transformers.AutoTokenizer.from_pretrained(f\"{self.dir_path}/tokenizer\", padding_side=\"right\")\n",
    "        else:\n",
    "            self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
    "        self.tokenizer.add_special_tokens({  \"pad_token\": \"<pad>\",\n",
    "                                             \"eos_token\": \"<end>\",\n",
    "                                             \"sep_token\": \"<sep>\"})\n",
    "        self.tokenizer.add_tokens([\"<bot>\"])\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.offline:\n",
    "            self.model = transformers.GPT2LMHeadModel.from_pretrained(f\"{self.dir_path}/model\")\n",
    "        else:\n",
    "            self.model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.model.eval()\n",
    "        self.model = self.model.to(self.device)\n",
    "        if self.print_info:\n",
    "            print(f\"loading weights from '{self.dir_path}/weights/model_state_{self.model_version}.pt'\")\n",
    "        self.model.load_state_dict(torch.load(f\"{self.dir_path}/weights/model_state_{self.model_version}.pt\", map_location=self.device))\n",
    "\n",
    "    def inference(self, prompt:str, clear_output=True, print_input=False, print_output=False):\n",
    "        user_input = prompt\n",
    "        if len(self.prompt) == 0:\n",
    "            self.prompt += f\"{prompt}\"\n",
    "        else:\n",
    "            self.prompt += f\"<sep>{prompt}\"\n",
    "        if print_input:\n",
    "            print(f\"{self.prompt}<bot>\")\n",
    "        prompt = self.tokenizer(f\"{self.prompt}<bot>\",  truncation=True, \n",
    "                                                        return_tensors=\"pt\", \n",
    "                                                        #max_length=self.max_length, \n",
    "                                                        padding=True)\n",
    "        X = prompt[\"input_ids\"].to(self.device)\n",
    "        a = prompt[\"attention_mask\"].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(X, attention_mask=a, \n",
    "                                            pad_token_id=self.tokenizer.pad_token_id,\n",
    "                                            do_sample=True, \n",
    "                                            max_length=self.max_length)  #*2\n",
    "        \n",
    "        if print_output:\n",
    "            _ = self.tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "            if type(_) == list and len(_) == 1:\n",
    "                _ = _[0]\n",
    "            print(_)\n",
    "\n",
    "        output = self.tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "        if type(output) == list and len(output) == 1:\n",
    "            output = output[0]\n",
    "\n",
    "        if clear_output:\n",
    "            start_idx = None\n",
    "            end_idx = None\n",
    "            for idx in range(0, len(output)-5):\n",
    "                cur_word = output[idx:idx+5]\n",
    "                if cur_word == \"<bot>\":\n",
    "                    start_idx = idx+5\n",
    "                elif cur_word in [\"<end>\", \"<pad>\"]:\n",
    "                    end_idx = idx\n",
    "\n",
    "                if type(start_idx) == int and type(end_idx) == int:\n",
    "                    break                \n",
    "\n",
    "            # check if cleaning worked, else try something else\n",
    "            if type(start_idx) == int and type(end_idx) == int:\n",
    "                output = output[start_idx:end_idx]\n",
    "            else:\n",
    "                if self.print_info:\n",
    "                    print(\"There are problem during clearing the output.\")\n",
    "\n",
    "                if \"<bot>\" in output:\n",
    "                    output = \"\".join(output.split(\"<bot>\")[1:])\n",
    "\n",
    "                if \"<end>\" in output:\n",
    "                    output = output.split(\"<end>\")[0]\n",
    "\n",
    "                if \"<pad>\" in output:\n",
    "                    output = output.split(\"<pad>\")[0]\n",
    "\n",
    "                if \"<sep>\" in output:\n",
    "                    output = output.replace(\"<sep>\", \". \")\n",
    "    \n",
    "\n",
    "        self.prompt += f\"<sep>{output}\"\n",
    "        return output\n",
    "\n",
    "    def set_topic(self, topic):\n",
    "        self.topic = topic\n",
    "\n",
    "    def save_history(self, path=\"./histories\", name=None, override=False):\n",
    "        if type(name) != str:\n",
    "            name = f\"chat {dt.now().strftime('%Y-%m-%d %H_%M')}.txt\"\n",
    "        if not name.endswith(\".txt\"):\n",
    "            name += \".txt\"\n",
    "        counter = 2\n",
    "        if override == False:\n",
    "            while name in os.listdir(path):\n",
    "                name = f\"{name} {counter}.txt\"\n",
    "        \n",
    "        with open(f\"{path}/{name}\", \"w\") as f:\n",
    "            f.write(self.history)\n",
    "        return name\n",
    "\n",
    "    def load_histories(self, path=\"./histories\"):\n",
    "        histories = []\n",
    "        for i in os.listdir(path):\n",
    "            if len(i) > 0:\n",
    "                if i.endswith(\".txt\"):\n",
    "                    histories += [\".\".join(i.split(\".\")[:-1])]\n",
    "                else:\n",
    "                    histories += [i]\n",
    "        return histories\n",
    "\n",
    "    def load_history(self, path, name):\n",
    "        self.reload()\n",
    "        chat = self.get_history(path, name)\n",
    "        self.prompt = \"\"\n",
    "        for i, text in enumerate(chat):\n",
    "            if i == 0:\n",
    "                self.prompt += f\"{text}\"\n",
    "            else:\n",
    "                self.prompt += f\"<sep>{text}\"\n",
    "\n",
    "    def get_history(self, path, name):\n",
    "        if not name.endswith(\"txt\"):\n",
    "            name += \".txt\"\n",
    "        try:\n",
    "            with open(f\"{path}/{name}\", \"r\") as f:\n",
    "                history = f.read()\n",
    "        except Exception:\n",
    "            name = \".\".join(name.split(\".\")[:-1])\n",
    "            with open(f\"{path}/{name}\", \"r\") as f:\n",
    "                history = f.read()\n",
    "\n",
    "        history = history.split(\"\\n\\n\")\n",
    "        return_history = []\n",
    "        for chat_idx in range(0, len(history)):\n",
    "            if len(history[chat_idx]) > 0:\n",
    "                if \":\" in history[chat_idx]:\n",
    "                    return_history += [\":\".join(history[chat_idx].split(\":\")[1:])]\n",
    "                else:\n",
    "                    return_history += [history[chat_idx]]\n",
    "        return return_history\n",
    "\n",
    "    def del_history(self, path, name):\n",
    "        if not name.endswith(\"txt\"):\n",
    "            name += \".txt\"\n",
    "\n",
    "        try:\n",
    "            os.remove(f\"{path}/{name}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"The file {file_path} are not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during deleting the file: {e}\")\n",
    "\n",
    "    def add_to_history(self, is_user, message):\n",
    "        if is_user:\n",
    "            self.history += f\"\\n\\nuser: {message}\"\n",
    "        else:\n",
    "            self.history += f\"\\n\\nbot: {message}\"\n",
    "\n",
    "    def save_model(self, empty_top_dir_path):\n",
    "        # model\n",
    "        model_path = f\"{empty_top_dir_path}/model\"\n",
    "        os.makedirs(model_path)\n",
    "        bot.model.save_pretrained(model_path)\n",
    "        # tokenizer\n",
    "        tokenizer_path = f\"{empty_top_dir_path}/tokenizer\"\n",
    "        os.makedirs(tokenizer_path)\n",
    "        bot.tokenizer.save_pretrained(tokenizer_path)\n",
    "        # weights\n",
    "        weights_path = f\"{empty_top_dir_path}/weights\"\n",
    "        os.makedirs(weights_path)\n",
    "        torch.save(model, f\"{weights_path}/model_state_{self.model_version}.pt\")\n",
    "        return model_path, tokenizer_path, weights_path \n",
    "\n",
    "    def reload(self):\n",
    "        self.history = \"\"\n",
    "        self.prompt = \"\"\n",
    "        self.set_device()\n",
    "        self.load_tokenizer()\n",
    "        self.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GOOGLE_COLAB:\n",
    "    path = \"/content/gdrive/My Drive/\"\n",
    "else:\n",
    "    path = \".\"\n",
    "\n",
    "bot = Calm_Bot(dir_path=path, print_info=True, offline=False, model_version=\"V6_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "c:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1363: UserWarning: Input length of input_ids is 1024, but `max_length` is set to 1024. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: hey<bot>\n",
      "See you later! I hope you had fun ^^\n"
     ]
    }
   ],
   "source": [
    "loop = 0\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"q\", \"quit\", \"e\", \"exit\", \"\", \"x\"]:\n",
    "        print(\"See you later! Bye\")\n",
    "        break\n",
    "    elif user_input in [\"restart\", \"new\"]:\n",
    "        loop = 0\n",
    "\n",
    "    print(\"Bot:\", bot.inference(user_input))\n",
    "    loop += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
