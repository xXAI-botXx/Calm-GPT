{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jWfzNlZXoUa"
      },
      "source": [
        "# Fine Tune GPT-2 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzL2cimNXoUb"
      },
      "source": [
        "Open Questions:\n",
        "- Is it useful to add \\<bot> statement as preparing step? -> yesss\n",
        "- Should one batch, one dialog?\n",
        "- Currently the dialogs are mixed, so only question and answer is paired right now\n",
        "    - How to fix?\n",
        "    - The batches?\n",
        "- Removing Bot answers?<br>\n",
        "    From:<br>\n",
        "    '<start> Create me a unique interactive story to calm with the topic: Ocean. <bot>:Ah, the ocean... <end>',<br>\n",
        "    \"<start> Ah, the ocean. Such a ... <end>\",<br>\n",
        "    \"<start> Yes, I can feel it...'<br>\n",
        "    <br>\n",
        "    to:<br>\n",
        "    '<start> Create me a unique interactive story to calm with the topic: Ocean. <bot>:Ah, the ocean... <end>',<br>\n",
        "    \"<start> Yes, I can feel it...'<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxvjNZgAXoUc"
      },
      "source": [
        "\n",
        "1. **Dialog-based Approach:**\n",
        "   - **One Batch, One Dialog:**\n",
        "     - Treat each dialog as a separate training example. This allows the model to learn the context and flow of individual conversations.\n",
        "     - Helps the model focus on capturing the nuances of each conversation independently.\n",
        "     - Useful if your storytelling involves short, distinct dialogs.\n",
        "\n",
        "   - **Inclusion of the Past:**\n",
        "     - You can include the past history within each dialog example. Concatenate the previous turns in the conversation to provide context.\n",
        "     - This helps the model understand the context and continuity of the ongoing dialog.\n",
        "     - Be mindful of the token limit, as GPT-2 has a maximum token limit, and longer sequences might get truncated.\n",
        "\n",
        "2. **Memory and Context:**\n",
        "   - GPT-2 has a limited context window due to its fixed input size. If the conversations are long, you might lose relevant information.\n",
        "   - Consider balancing the length of your input sequences to ensure the model can capture essential details.\n",
        "\n",
        "3. **Dynamic Context Window:**\n",
        "   - Instead of a fixed history length, you could use a sliding window approach.\n",
        "   - Maintain a dynamic context window that moves along the conversation, incorporating the most recent interactions.\n",
        "\n",
        "4. **Experiment and Evaluate:**\n",
        "   - It's often beneficial to experiment with different approaches to see what works best for your specific use case.\n",
        "   - Conduct thorough evaluations using validation data to ensure the model is learning effectively and providing desired responses.\n",
        "\n",
        "5. **Training Strategies:**\n",
        "   - Experiment with hyperparameters like learning rate, batch size, and the number of training epochs to fine-tune the model effectively.\n",
        "   - Monitor the model's performance on both training and validation sets.\n",
        "\n",
        "Preprocess: handling tokenization, special tokens, and managing the context window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi2RT8J7XoUc"
      },
      "source": [
        "Hint: Use the dialogs.txt file to train the model on google colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik25yGpAyDTB"
      },
      "source": [
        "### System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-csAFyhVyWZ3",
        "outputId": "5d039fed-1f19-4df7-80c5-c17713917bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpP_Sm9KyFmp",
        "outputId": "d0949d81-2e7b-4f94-ed01-e9dafce7d4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM4OxmVbyUUu",
        "outputId": "c7eabf6f-6150-4b39-8ba8-750ccae066b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgfbuj5Szs8n",
        "outputId": "730e14cf-31cc-4c97-a443-79e0f8d7ac0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: AuthenticAMD\n",
            "cpu family\t: 23\n",
            "model\t\t: 49\n",
            "model name\t: AMD EPYC 7B12\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2249.998\n",
            "cache size\t: 512 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n",
            "bugs\t\t: sysret_ss_attrs null_seg spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\n",
            "bogomips\t: 4499.99\n",
            "TLB size\t: 3072 4K pages\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 48 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: AuthenticAMD\n",
            "cpu family\t: 23\n",
            "model\t\t: 49\n",
            "model name\t: AMD EPYC 7B12\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2249.998\n",
            "cache size\t: 512 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n",
            "bugs\t\t: sysret_ss_attrs null_seg spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\n",
            "bogomips\t: 4499.99\n",
            "TLB size\t: 3072 4K pages\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 48 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6YYmZ7rzwIJ",
        "outputId": "78102055-2281-4592-da8a-b66d93f3dcb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13290480 kB\n",
            "MemFree:         7239128 kB\n",
            "MemAvailable:   11348988 kB\n",
            "Buffers:          363924 kB\n",
            "Cached:          3935072 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           639376 kB\n",
            "Inactive:        5135192 kB\n",
            "Active(anon):       1084 kB\n",
            "Inactive(anon):  1478976 kB\n",
            "Active(file):     638292 kB\n",
            "Inactive(file):  3656216 kB\n",
            "Unevictable:           4 kB\n",
            "Mlocked:               4 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:              2632 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:       1473492 kB\n",
            "Mapped:           474232 kB\n",
            "Shmem:              4484 kB\n",
            "KReclaimable:     112584 kB\n",
            "Slab:             149860 kB\n",
            "SReclaimable:     112584 kB\n",
            "SUnreclaim:        37276 kB\n",
            "KernelStack:        4420 kB\n",
            "PageTables:        30812 kB\n",
            "SecPageTables:         0 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6645240 kB\n",
            "Committed_AS:    2495140 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       10656 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1080 kB\n",
            "HardwareCorrupted:     0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "Unaccepted:            0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:       58168 kB\n",
            "DirectMap2M:     4132864 kB\n",
            "DirectMap1G:    11534336 kB\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/meminfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhd3wPF2XoUc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "697uWwk6XoUd"
      },
      "outputs": [],
      "source": [
        "#!python -m pip install torch\n",
        "#!python -m pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MrZaWIOKXoUd"
      },
      "outputs": [],
      "source": [
        "#from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from datetime import datetime as dt\n",
        "\n",
        "import json\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbGx0tsncS8S",
        "outputId": "b5584680-e8bc-4110-ce8f-71d21058d32c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'dialogs.txt', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "os.listdir(\"./\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgi402IpXoUd"
      },
      "source": [
        "### Load and Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VFiQ05wnXoUd"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"./model/model.pth\"\n",
        "MODEL_WEIGHT_PATH = \"./model/model_weights.pth\"\n",
        "ONNX_PATH = \"./model/model.onnx\"\n",
        "MAX_LENGTH = 1024   #\"auto\"\n",
        "# \".pt\", \".pth\", \".pkl\", or \".h5\"\n",
        "\n",
        "class Dialog_Data(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, data_dir_path=\"./data\", read_one_file=False, should_save_as_one_file=True):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data_dir_path = data_dir_path\n",
        "        self.read_data(data_dir_path, read_one_file, should_save_as_one_file)\n",
        "\n",
        "    def read_data(self, data_dir_path, read_one_file, should_save_as_one_file=True):\n",
        "        global MAX_LENGTH\n",
        "\n",
        "        data = []\n",
        "        conversations = []\n",
        "        if read_one_file:\n",
        "            with open(\"./dialogs.txt\", \"r\", encoding=\"latin1\") as f:\n",
        "                raw = f.read()\n",
        "            for dialog in raw.split(\"#/\"):\n",
        "                cur_conversation = []\n",
        "                for sentence in dialog.split(\";\"):\n",
        "                    data += [sentence]\n",
        "                    cur_conversation += [sentence]\n",
        "                conversations += [(cur_conversation)]\n",
        "        else:\n",
        "            for dialog in os.listdir(self.data_dir_path):\n",
        "                    with open(f\"{self.data_dir_path}/{dialog}\", \"r\") as f:\n",
        "                        cur_conversation = []\n",
        "                        for idx, line in enumerate(f.read().split(\"\\n\")):\n",
        "                            content = \":\".join(line.split(\":\")[1:]).strip()\n",
        "                            if len(content) > 0:\n",
        "                                data += [content]\n",
        "                                cur_conversation += [content]\n",
        "                    conversations += [(cur_conversation)]\n",
        "            if should_save_as_one_file:\n",
        "                save_data = \"\"\n",
        "                for idx_1, dialog in enumerate(conversations):\n",
        "                    if idx_1 > 0:\n",
        "                        save_data += \"#/\"\n",
        "\n",
        "                    for idx_2, elem in enumerate(dialog):\n",
        "                        if idx_2 == 0:\n",
        "                            save_data += f\"{elem}\"\n",
        "                        else:\n",
        "                            save_data += f\";{elem}\"\n",
        "                    with open(\"./dialogs.txt\", \"w\") as f:\n",
        "                        f.write(save_data)\n",
        "\n",
        "        # add markers and trim\n",
        "        X = []\n",
        "        y = []\n",
        "        is_conversation_beginning = []\n",
        "        for cur_conversation in conversations:\n",
        "            for idx in range(0, len(cur_conversation)-1, 2):\n",
        "                if idx == 0:\n",
        "                    is_conversation_beginning += [1]\n",
        "                else:\n",
        "                    is_conversation_beginning += [0]\n",
        "                X += [cur_conversation[idx]]\n",
        "                y += [cur_conversation[idx+1]]\n",
        "\n",
        "        self.conversations = conversations\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.is_conversation_beginning = is_conversation_beginning\n",
        "\n",
        "        # encoded_data = self.tokenizer(self.X, truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH, padding=\"max_length\") # max_length=40, padding=\"max_length\"\n",
        "        # self.X_encoded = encoded_data['input_ids']\n",
        "        # self.X_attention_mask = encoded_data['attention_mask']\n",
        "\n",
        "        encoded_data = self.tokenizer(self.y, truncation=False, return_tensors=\"pt\", max_length=MAX_LENGTH, padding=\"max_length\")\n",
        "        self.y_encoded =  encoded_data['input_ids']\n",
        "        self.y_attention_mask = encoded_data['attention_mask']\n",
        "\n",
        "    def get_context(self, idx):\n",
        "        with_context = \"\"\n",
        "        cur_idx = idx\n",
        "        while cur_idx >= 0:\n",
        "            if cur_idx == idx:\n",
        "                with_context += f\"<bot>{self.y[cur_idx]}\"  # <end>\n",
        "                with_context = f\"{self.X[cur_idx]}{with_context}\"\n",
        "            else:\n",
        "                with_context = f\"{self.y[cur_idx]}<sep>{with_context}\"\n",
        "                with_context = f\"{self.X[cur_idx]}<sep>{with_context}\"\n",
        "            if self.is_conversation_beginning[cur_idx] == 1:\n",
        "                break\n",
        "            cur_idx -= 1\n",
        "        #with_context = f\"<start>{with_context}\"\n",
        "        return with_context\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.get_context(idx)\n",
        "        encoded_data = self.tokenizer(X, truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH, padding=\"max_length\")\n",
        "        return (encoded_data['input_ids'], encoded_data['attention_mask'])#, self.y_encoded[idx])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ONd2dOaXoUe",
        "outputId": "5c02f4e0-bedc-494e-bc7b-7e921ceaf9d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
        "tokenizer.add_special_tokens({  \"pad_token\": \"<pad>\",\n",
        "                                # \"bos_token\": \"<start>\",\n",
        "                                # \"eos_token\": \"<end>\",\n",
        "                                \"sep_token\": \"<sep>\"})\n",
        "tokenizer.add_tokens([\"<bot>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "r2qOzeAAXoUe"
      },
      "outputs": [],
      "source": [
        "data = Dialog_Data(tokenizer=tokenizer, read_one_file=True, should_save_as_one_file=True)\n",
        "data = DataLoader(data, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9Gto4PnaHK_",
        "outputId": "d9f0f644-6436-4877-f5c4-5f2c60e2349b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey, I've been feeling really down lately. I just can't seem to find any motivation or purpose in my life. \n",
            "\n",
            "What was the story about? \n",
            "\n",
            "That's really inspiring. But how did he manage to find motivation and purpose in his life? \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# check the data\n",
        "[print(i, \"\\n\") for i in data.dataset.X[:3]];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "hrAiJqjcHVtq",
        "outputId": "5eed5331-7f87-4f96-8bd1-5ca1456e1f1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hey, I've been feeling really down lately. I just can't seem to find any motivation or purpose in my life.<sep>I completely understand how you feel. I went through a similar phase a while back. I had lost all sense of direction and felt like my life lacked purpose. But then I came across this incredible story that really inspired me.<sep>What was the story about?<sep>It was about a man named Nick Vujicic. He was born without arms and legs, and faced numerous challenges and obstacles throughout his life. Despite all that, he never let his disabilities define him. Instead, he used his setbacks as fuel to achieve incredible things. He became a motivational speaker, inspiring millions of people around the world.<sep>That's really inspiring. But how did he manage to find motivation and purpose in his life?<sep>Well, Nick didn't let his circumstances determine his happiness or success. He believed that true happiness and purpose come from within, and he focused on developing a positive mindset. He found joy in helping others and making a difference in their lives. He embraced his unique situation and used it as a platform to inspire others.<sep>That's amazing. But I still struggle to find that inner motivation and purpose in my own life.<bot>I think the key is to start small and take little steps towards finding what truly makes you happy. Explore different interests, set goals for yourself, and surround yourself with positive and supportive people. Remember, everyone's journey is different, and it's okay to take your time to discover your passion and purpose. Believe in yourself and your abilities, and never give up.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "data.dataset.get_context(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "V1EZKQOxHVtr",
        "outputId": "73e170c0-29f6-43b9-b277-8637a945e476"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hey, I've been feeling really overwhelmed lately and I think I might have Separation Anxiety Disorder.<bot>Oh, I'm sorry to hear that. What exactly are you experiencing?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "data.dataset.get_context(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "xPJTFKhnHVtr",
        "outputId": "86b91a72-8151-431e-cf16-03037964457c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Good afternoon, Doctor. Thank you for seeing me today.<sep>Good afternoon. It's my pleasure. How can I assist you?<sep>I've been feeling really overwhelmed lately. I constantly feel anxious and have trouble sleeping.<sep>I'm sorry to hear that. When did you first start noticing these symptoms?<sep>It's been going on for a few months now. It started after a major life event - a job loss and the end of a long-term relationship.<bot>Those are significant stressors indeed. How do you cope with these feelings of anxiety?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "data.dataset.get_context(203)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9Ns1dy6adDf",
        "outputId": "2ca5fd95-af60-4597-dcce-60da0c0772ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11149"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "len(data.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZcjijhTTHVtr"
      },
      "outputs": [],
      "source": [
        "# # Test saved dialogs in one file\n",
        "# counter = 0\n",
        "# with open(\"./dialogs.txt\", \"r\") as f:\n",
        "#     dialogs = f.read()\n",
        "# print(f\"Dialogs amount: {len(os.listdir('./data'))}\")\n",
        "# print(f\"In one file dialogs amount: {len(dialogs.split('#'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tAGX1Qq8XoUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1a03d5-6f0c-4de3-e2bb-9d74da8fc4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:\n",
            "Decoded: <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>Hey<bot>Hey, how are you?\n",
            "Encoded: tensor([[50257, 50257, 50257,  ...,   389,   345,    30]])\n",
            "<class 'torch.Tensor'>\n",
            "1\n",
            "AttentionMask:\n",
            " tensor([[0, 0, 0,  ..., 1, 1, 1]])\n",
            "<class 'torch.Tensor'>\n",
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2788"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "BATCH_AMOUNT = 0\n",
        "for X, a in data:\n",
        "    BATCH_AMOUNT += 1\n",
        "    if BATCH_AMOUNT == 5:\n",
        "        print(\"X:\")\n",
        "        print(\"Decoded:\", tokenizer.decode(X[0][0]))\n",
        "        print(\"Encoded:\", X[0])\n",
        "        print(type(X[0]))\n",
        "        print(len(X[0]))\n",
        "        print(\"AttentionMask:\\n\", a[0])\n",
        "        print(type(a[0]))\n",
        "        print(len(a[0]))\n",
        "        # print(\"Target:\\nDecoded:\", tokenizer.decode(y[0][0]))\n",
        "        # print(\"Encoded:\", y[0])\n",
        "        # print(type(y[0]))\n",
        "        # print(len(y[0]))\n",
        "\n",
        "BATCH_AMOUNT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fm_F-1MXoUe"
      },
      "source": [
        "### Load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550,
          "referenced_widgets": [
            "7a71e74373204704893e03152a3adb15",
            "2f02404df4924085a9eb880f446cab11",
            "8bfc8294844342869b48dddfde00bece",
            "ea2dc65c28ff4f5bbe620f132d955b8f",
            "a3e2f4a6fea64d579fd511da6586cf7a",
            "a1cd94e55119456e8236aef1972103c4",
            "2109710001d24117855f2985c3700745",
            "cf4f95bee22f42c1b6a3f45341d66a66",
            "f14116aadd304af699ed952cae52e336",
            "bc508df866a14dc7abfca30666b1643f",
            "398fcb64b2f145bba857b98e6edb0eb8",
            "2b30aff0c4e74cb6b98d252d74ce62b7",
            "90238a2caf39423a8c9364a64383bdb2",
            "41196afad10e4d33aa3f587aaf09e72d",
            "f5d602e559dd40a8bf7bc61ee48004e0",
            "d28882790b2a4eb1aae16497d0cbb0d9",
            "b72281b6994b418e8f5b9e19da74c61e",
            "e3722074641746d9b394b7621f13e20c",
            "73538fa372724716875c5163296721b5",
            "612e5597fd904edf90b3132bc85b602c",
            "ef08dc79009e4f0d8052c6e45b20428d",
            "5b770a1b33414cda965bd447b196f79f"
          ]
        },
        "id": "eSDnf19QXoUe",
        "outputId": "1acee8a1-568f-489a-b1ce-bf37ef08bc99"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a71e74373204704893e03152a3adb15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b30aff0c4e74cb6b98d252d74ce62b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50260, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# config = transformers.GPT2Config.from_pretrained(\"gpt2\")\n",
        "# config.do_sample = config.task_specific_params['text-generation']['do_sample']\n",
        "# config.max_length = MAX_LENGTH #config.task_specific_params['text-generation']['max_length']\n",
        "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex5nw1oJYQpx",
        "outputId": "526a46f3-00ca-4da4-bc8b-a315315cb9ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3F0udytjYUST"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt7xXSLpXoUf"
      },
      "source": [
        "### First test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "z5jeBTWOXeyW"
      },
      "outputs": [],
      "source": [
        "def inference(prompt:str, model, tokenizer, device, padding, clear_output=True):\n",
        "    model.eval()\n",
        "    prompt = f\"{prompt}<bot>\"\n",
        "    prompt = tokenizer(prompt, return_tensors=\"pt\", padding=padding)\n",
        "    X = prompt[\"input_ids\"].to(device)\n",
        "    a = prompt[\"attention_mask\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(X, attention_mask=a, pad_token_id=tokenizer.pad_token_id,\n",
        "                                                        do_sample=True, max_length=MAX_LENGTH)\n",
        "\n",
        "    if clear_output:\n",
        "        output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        output = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "\n",
        "    if type(output) == list and len(output) == 1:\n",
        "        output = output[0]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KSqQU8y1XmFZ",
        "outputId": "b2d54bc1-82bd-41fb-f076-9e0f8dcf011b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hey, I'm feeling not so good.<bot>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "inference(prompt=\"Hey, I'm feeling not so good.\", model=model, tokenizer=tokenizer,\n",
        "                                                        device=device, padding=\"max_length\", clear_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "1wIUfCozcxi5",
        "outputId": "2a2a63da-f88e-4379-b794-c403a1824e64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>Hey, I've been feeling really down lately.<bot><pad>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "inference(prompt=\"Hey, I've been feeling really down lately.\", model=model, tokenizer=tokenizer, device=device, padding=\"max_length\", clear_output=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0zGPe1pXoUf"
      },
      "source": [
        "### Fine Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "IDs_W4SUwA0B"
      },
      "outputs": [],
      "source": [
        "def print_time_information(start, end=None, total_seconds=None, text=\"Total training time:\", should_print=True):\n",
        "    if type(total_seconds) is type(None):\n",
        "        if type(end) is type(None):\n",
        "            end = dt.now()\n",
        "        total_seconds = abs((start-end).total_seconds())\n",
        "\n",
        "    minutes, seconds = divmod(total_seconds, 60)\n",
        "    hours, minutes = divmod(minutes, 60)\n",
        "    days, hours = divmod(hours, 24)\n",
        "    res = f\"{text}\\n    -> {int(days)} Days\\n    -> {int(hours)} Hours\\n    -> {int(minutes)} Minutes\\n    -> {int(seconds)} Seconds\"\n",
        "    if should_print:\n",
        "        print(res)\n",
        "    return res\n",
        "\n",
        "def calculate_train_duration(epoch_start, batch_amount, epochs, cur_epoch):\n",
        "    \"\"\"\n",
        "    Call this function once after the first batch every epoch.\n",
        "    \"\"\"\n",
        "    cur_epoch += 1\n",
        "    now = dt.now()\n",
        "    duration_one_batch = abs((epoch_start-now).total_seconds())\n",
        "    duration_for_one_epoch = duration_one_batch * batch_amount\n",
        "    epochs_left = (epochs - cur_epoch) + 1    # current epoch also have to run\n",
        "    predicted_training_duration = epochs_left * duration_for_one_epoch\n",
        "    res = f\"{'-'*16}\\n\"\n",
        "    text = f\"Training will need about following time for {epochs_left} epochs:\"\n",
        "    res += print_time_information(start=epoch_start, total_seconds=predicted_training_duration, text=text, should_print=False)\n",
        "    res += f\"\\n{'-'*16}\"\n",
        "    print(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "hqc6pXHvXoUf",
        "outputId": "9c5d26c3-c811-4087-e74a-18a41be6edbc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "GPT2LMHeadModel.forward() got an unexpected keyword argument 'max_length'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-1c2a5e53b568>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss_hist\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: GPT2LMHeadModel.forward() got an unexpected keyword argument 'max_length'"
          ]
        }
      ],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "epochs = 6\n",
        "\n",
        "loss_hist = []\n",
        "steps = 0\n",
        "\n",
        "solutions = []\n",
        "solutions_cleared = []\n",
        "\n",
        "start = dt.now()\n",
        "\n",
        "with open(\"./log.txt\", \"w\") as f:\n",
        "    f.write(f\"Log File for Training Calm Chatbot {start.strftime('%d.%m.%Y - %H:%M:%S')}\")\n",
        "\n",
        "for cur_epoch in range(0, epochs):\n",
        "    model.train()\n",
        "    epoch_start = dt.now()\n",
        "    new_epoch = True\n",
        "    for input_ids, attention_masks in data:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_masks = attention_masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(input_ids, attention_mask=attention_masks, labels=input_ids).loss\n",
        "        loss_hist += [loss.item()]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "\n",
        "        if new_epoch:\n",
        "            time_prediction = calculate_train_duration(epoch_start, BATCH_AMOUNT, epochs, cur_epoch)\n",
        "            new_epoch = False\n",
        "\n",
        "            with open(\"./log.txt\", \"a\") as f:\n",
        "                f.write(f\"\\n\\n{time_prediction}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"./model_state_V4_{cur_epoch}.pt\")\n",
        "    epoch_info = f'Epoch {cur_epoch+1}/{epochs}, Training Loss: {loss.item():.4f}, Steps: {steps}, Current Time:{dt.now().strftime(\"%H:%M:%S\")}'\n",
        "    print(epoch_info)\n",
        "    test_prompt = inference(prompt=\"Hey, I'm feeling not so good.\", model=model, tokenizer=tokenizer,\n",
        "                                                        device=device, padding=True, clear_output=False)\n",
        "    solutions += [test_prompt]\n",
        "    solutions_cleared += [inference(prompt=\"Hey, I'm feeling not so good.\", model=model, tokenizer=tokenizer,\n",
        "                                                        device=device, padding=True, clear_output=True)]\n",
        "\n",
        "    with open(\"./log.txt\", \"a\") as f:\n",
        "        f.write(f\"\\n\\n{epoch_info}\\n\\nTest-Prompt:\\n{test_prompt}\")\n",
        "\n",
        "    # allocate memory\n",
        "    #gc.collect()\n",
        "    # if device.type == \"cuda\":\n",
        "    #     torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HprXXm0SHVtt"
      },
      "outputs": [],
      "source": [
        "print_time_information(start)\n",
        "\n",
        "# plot loss\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "ax.plot(np.arange(len(loss_hist)-1), loss_hist[1], label='Loss')\n",
        "ax.set_xlabel('Learning progress')\n",
        "ax.set_ylabel('Loss (normalized mean absolute error)')\n",
        "ax.set_title('Loss over time')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "\n",
        "# save step solution-predictions:\n",
        "with open(\"./result_per_epoch.txt\", \"w\") as f:\n",
        "    res = \"\"\n",
        "    for i, cur_res in enumerate(solutions, start=1):\n",
        "        res += f\"\\n{'-'*16}\\n{i:02d}. Epoch:\\n{cur_res}\"\n",
        "    f.write(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePOQnOZXk29I"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('./loss_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(loss_hist, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atJ7S98hkxPv"
      },
      "outputs": [],
      "source": [
        "# plot loss\n",
        "OFFSET = 0\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "ax.plot(np.arange(len(loss_hist)-OFFSET), loss_hist[OFFSET:], label='Loss')\n",
        "ax.set_xlabel('Learning progress')\n",
        "ax.set_ylabel('Loss (normalized mean absolute error)')\n",
        "ax.set_title('Loss over time')\n",
        "ax.legend()\n",
        "ax.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHQkbfDfmJqv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.ticker as mticker\n",
        "\n",
        "OFFSET = 0\n",
        "loss_series = pd.Series(loss_hist[OFFSET:])\n",
        "\n",
        "# Wenden Sie das gleitende Fenster an\n",
        "window_size = 1000  # Gre des gleitenden Fensters\n",
        "loss_rolling = loss_series.rolling(window=window_size).mean()\n",
        "\n",
        "# Zeichnen Sie die ursprnglichen und gegltteten Daten\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "#ax.plot(np.arange(len(loss_hist)), loss_hist, label='Original Loss')\n",
        "ax.plot(np.arange(len(loss_series.index)), loss_rolling, label='Smoothed Loss', linewidth=2)\n",
        "ax.set_xlabel('Learning progress')\n",
        "ax.set_ylabel('Loss (normalized mean absolute error)')\n",
        "ax.set_title('Loss over time')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "# formatter = mticker.ScalarFormatter()\n",
        "# formatter.set_scientific(False)\n",
        "# ax.yaxis.set_major_formatter(formatter)\n",
        "# ax.set_yscale('log')\n",
        "\n",
        "plt.savefig(\"./loss.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnfKTdk5XoUf"
      },
      "source": [
        "### Save model\n",
        "\n",
        "-> Propably save the model in a extra repository/branch and provide it as python module<br>\n",
        "-> Is model very big?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8M7Q0aUXoUf"
      },
      "source": [
        "save only weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9QFdpqfXoUf"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), MODEL_WEIGHT_PATH)\n",
        "\n",
        "# loading\n",
        "# config = transformers.GPT2Config.from_pretrained(\"gpt2\")\n",
        "# config.max_length = MAX_LENGTH #config.task_specific_params['text-generation']['max_length']\n",
        "# model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "# model.load_state_dict(torch.load(MODEL_WEIGHT_PATH))\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNbmw4pDXoUf"
      },
      "source": [
        "save whole model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6gPFu3RXoUf"
      },
      "outputs": [],
      "source": [
        "torch.save(model, MODEL_PATH)\n",
        "\n",
        "# loading\n",
        "# model = torch.load(MODEL_PATH)\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgWcVcIhtQ73"
      },
      "source": [
        "save in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "idmgWLSotQlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee6113d-c5c5-4094-ba10-15009e58482a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# path = \"/content/gdrive/My Drive/model.pt\"\n",
        "# torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x56vJ4Clfoci"
      },
      "source": [
        "---\n",
        "### Ressources:\n",
        "\n",
        "- https://www.toolify.ai/ai-news/finetuning-gpt2-for-conversational-chatbots-10476\n",
        "- https://huggingface.co/docs/transformers/model_doc/gpt2\n",
        "- [PyTorch kompakt](https://www.thalia.de/shop/home/artikeldetails/A1062166688)\n",
        "- https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
        "- https://github.com/itsuncheng/fine-tuning-GPT2/tree/master\n",
        "- https://www.kaggle.com/code/pinooxd/gpt2-chatbot/notebook\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7a71e74373204704893e03152a3adb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f02404df4924085a9eb880f446cab11",
              "IPY_MODEL_8bfc8294844342869b48dddfde00bece",
              "IPY_MODEL_ea2dc65c28ff4f5bbe620f132d955b8f"
            ],
            "layout": "IPY_MODEL_a3e2f4a6fea64d579fd511da6586cf7a"
          }
        },
        "2f02404df4924085a9eb880f446cab11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1cd94e55119456e8236aef1972103c4",
            "placeholder": "",
            "style": "IPY_MODEL_2109710001d24117855f2985c3700745",
            "value": "model.safetensors: 100%"
          }
        },
        "8bfc8294844342869b48dddfde00bece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf4f95bee22f42c1b6a3f45341d66a66",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f14116aadd304af699ed952cae52e336",
            "value": 548105171
          }
        },
        "ea2dc65c28ff4f5bbe620f132d955b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc508df866a14dc7abfca30666b1643f",
            "placeholder": "",
            "style": "IPY_MODEL_398fcb64b2f145bba857b98e6edb0eb8",
            "value": " 548M/548M [00:05&lt;00:00, 73.5MB/s]"
          }
        },
        "a3e2f4a6fea64d579fd511da6586cf7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1cd94e55119456e8236aef1972103c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2109710001d24117855f2985c3700745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf4f95bee22f42c1b6a3f45341d66a66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f14116aadd304af699ed952cae52e336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc508df866a14dc7abfca30666b1643f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "398fcb64b2f145bba857b98e6edb0eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b30aff0c4e74cb6b98d252d74ce62b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90238a2caf39423a8c9364a64383bdb2",
              "IPY_MODEL_41196afad10e4d33aa3f587aaf09e72d",
              "IPY_MODEL_f5d602e559dd40a8bf7bc61ee48004e0"
            ],
            "layout": "IPY_MODEL_d28882790b2a4eb1aae16497d0cbb0d9"
          }
        },
        "90238a2caf39423a8c9364a64383bdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b72281b6994b418e8f5b9e19da74c61e",
            "placeholder": "",
            "style": "IPY_MODEL_e3722074641746d9b394b7621f13e20c",
            "value": "generation_config.json: 100%"
          }
        },
        "41196afad10e4d33aa3f587aaf09e72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73538fa372724716875c5163296721b5",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_612e5597fd904edf90b3132bc85b602c",
            "value": 124
          }
        },
        "f5d602e559dd40a8bf7bc61ee48004e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef08dc79009e4f0d8052c6e45b20428d",
            "placeholder": "",
            "style": "IPY_MODEL_5b770a1b33414cda965bd447b196f79f",
            "value": " 124/124 [00:00&lt;00:00, 2.79kB/s]"
          }
        },
        "d28882790b2a4eb1aae16497d0cbb0d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b72281b6994b418e8f5b9e19da74c61e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3722074641746d9b394b7621f13e20c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73538fa372724716875c5163296721b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "612e5597fd904edf90b3132bc85b602c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef08dc79009e4f0d8052c6e45b20428d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b770a1b33414cda965bd447b196f79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}