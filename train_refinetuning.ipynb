{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jWfzNlZXoUa"
      },
      "source": [
        "# Second Fine Tune GPT-2 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi2RT8J7XoUc"
      },
      "source": [
        "Hint: Use the dialogs.txt file to train the model on google colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIQLfKYOzqqv",
        "outputId": "b0bb1c28-2b24-4bbd-a5a1-135169544faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EU4Ft5fCzqqw"
      },
      "outputs": [],
      "source": [
        "def save_in_google(file_name, obj=None):\n",
        "    try:\n",
        "        google_path = \"/content/gdrive/My Drive/\"\n",
        "        cur_path = f\"{google_path}{file_name}\"\n",
        "        if file_name.endswith(\".pt\") or file_name.endswith(\".pth\"):\n",
        "            torch.save(model.state_dict(), cur_path)\n",
        "        elif file_name.endswith(\".pkl\"):\n",
        "            with open(cur_path, 'wb') as f:\n",
        "                pickle.dump(obj, f)\n",
        "        elif file_name.endswith(\".png\"):\n",
        "            plt.savefig(cur_path)\n",
        "        elif file_name.endswith(\".txt\"):\n",
        "            with open(cur_path, \"w\") as f:\n",
        "                f.write(obj)\n",
        "\n",
        "        print(f\"Succefull saved {file_name} in google ({dt.now().strftime('%d.%m.%Y - %H:%M:%S')})\")\n",
        "    except Exception:\n",
        "        print(f\"Failed to save {file_name} in google ({dt.now().strftime('%d.%m.%Y - %H:%M:%S')})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik25yGpAyDTB"
      },
      "source": [
        "### System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-csAFyhVyWZ3",
        "outputId": "ec67c8d9-bfa6-4c17-c5ef-da82c7163c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpP_Sm9KyFmp",
        "outputId": "07eee627-8012-47cf-c58c-4c8fad4e4dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan  6 01:16:32 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0              25W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM4OxmVbyUUu",
        "outputId": "1726efbd-22a6-4957-fbad-44c8791d4007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgfbuj5Szs8n",
        "outputId": "f2899031-b070-4bcc-d7f5-323fa2db5957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs mmio_stale_data retbleed\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs mmio_stale_data retbleed\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6YYmZ7rzwIJ",
        "outputId": "baa42b19-a81e-437e-eafc-f46fdef9f25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13290472 kB\n",
            "MemFree:         6992168 kB\n",
            "MemAvailable:   11895780 kB\n",
            "Buffers:          387116 kB\n",
            "Cached:          4671384 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           943320 kB\n",
            "Inactive:        4979076 kB\n",
            "Active(anon):       1220 kB\n",
            "Inactive(anon):   864308 kB\n",
            "Active(file):     942100 kB\n",
            "Inactive(file):  4114768 kB\n",
            "Unevictable:          16 kB\n",
            "Mlocked:              16 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:              2104 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        861796 kB\n",
            "Mapped:           324828 kB\n",
            "Shmem:              1624 kB\n",
            "KReclaimable:     172312 kB\n",
            "Slab:             223216 kB\n",
            "SReclaimable:     172312 kB\n",
            "SUnreclaim:        50904 kB\n",
            "KernelStack:        5632 kB\n",
            "PageTables:        13480 kB\n",
            "SecPageTables:         0 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6645236 kB\n",
            "Committed_AS:    2902396 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       76216 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1088 kB\n",
            "HardwareCorrupted:     0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "Unaccepted:            0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      197432 kB\n",
            "DirectMap2M:     2945024 kB\n",
            "DirectMap1G:    12582912 kB\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/meminfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhd3wPF2XoUc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "697uWwk6XoUd"
      },
      "outputs": [],
      "source": [
        "#!python -m pip install torch\n",
        "#!python -m pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MrZaWIOKXoUd"
      },
      "outputs": [],
      "source": [
        "#from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from datetime import datetime as dt\n",
        "\n",
        "import json\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbGx0tsncS8S",
        "outputId": "316c83a9-899e-4307-e086-92f2d47e8dbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'model',\n",
              " 'model_state_V5_1.pt',\n",
              " '.ipynb_checkpoints',\n",
              " 'gdrive',\n",
              " 'dialogs.txt',\n",
              " 'log.txt',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "os.listdir(\"./\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgi402IpXoUd"
      },
      "source": [
        "### Load and Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t0EAGrk6zqqz"
      },
      "outputs": [],
      "source": [
        "start_sentences = [\n",
        "    \"Create me a unique interactive story to calm with the topic:\",\n",
        "    \"Generate a unique interactive story for relaxation on the topic:\",\n",
        "    \"Craft a unique interactive narrative to soothe with the theme:\",\n",
        "    \"Design a unique interactive tale for calming with the subject:\",\n",
        "    \"Produce a unique interactive saga to unwind with the topic:\",\n",
        "    \"Compose a unique interactive chronicle for tranquility with the theme:\",\n",
        "    \"Formulate a unique interactive anecdote to de-stress with the subject:\",\n",
        "    \"Devise a distinctive interactive fable for serenity with the topic:\",\n",
        "    \"Invent a one-of-a-kind interactive parable for peace with the theme:\",\n",
        "    \"Create an exclusive interactive yarn for tranquility with the subject:\",\n",
        "    \"Form a novel interactive narrative for calmness with the topic:\",\n",
        "    \"Construct a unique interactive tale for relaxation with the theme:\",\n",
        "    \"Develop an original interactive story for soothing with the subject:\",\n",
        "    \"Visualize me following topic:\",\n",
        "    \"Picture me following subject:\",\n",
        "    \"Imagine me pursuing theme:\",\n",
        "    \"Envision me tracking topic:\",\n",
        "    \"See me following subject:\",\n",
        "    \"Conceive me chasing theme:\",\n",
        "    \"Create me a unique interactive story to calm with the topic\",\n",
        "    \"Generate a unique interactive story for relaxation on the topic\",\n",
        "    \"Craft a unique interactive narrative to soothe with the theme\",\n",
        "    \"Design a unique interactive tale for calming with the subject\",\n",
        "    \"Produce a unique interactive saga to unwind with the topic\",\n",
        "    \"Compose a unique interactive chronicle for tranquility with the theme\",\n",
        "    \"Formulate a unique interactive anecdote to de-stress with the subject\",\n",
        "    \"Devise a distinctive interactive fable for serenity with the topic\",\n",
        "    \"Invent a one-of-a-kind interactive parable for peace with the theme\",\n",
        "    \"Create an exclusive interactive yarn for tranquility with the subject\",\n",
        "    \"Form a novel interactive narrative for calmness with the topic\",\n",
        "    \"Construct a unique interactive tale for relaxation with the theme\",\n",
        "    \"Develop an original interactive story for soothing with the subject\",\n",
        "    \"Visualize me following topic\",\n",
        "    \"Picture me following subject\",\n",
        "    \"Imagine me pursuing theme\",\n",
        "    \"Envision me tracking topic\",\n",
        "    \"See me following subject\",\n",
        "    \"Conceive me chasing theme\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VFiQ05wnXoUd"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"./model/model.pth\"\n",
        "MODEL_WEIGHT_PATH = \"./model/model_weights.pth\"\n",
        "ONNX_PATH = \"./model/model.onnx\"\n",
        "MAX_LENGTH = 1024   #\"auto\"\n",
        "# \".pt\", \".pth\", \".pkl\", or \".h5\"\n",
        "\n",
        "class Dialog_Data(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, data_dir_path=\"./data\", read_one_file=False, should_save_as_one_file=True):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data_dir_path = data_dir_path\n",
        "        self.read_data(data_dir_path, read_one_file, should_save_as_one_file)\n",
        "\n",
        "    def read_data(self, data_dir_path, read_one_file, should_save_as_one_file=True):\n",
        "        global MAX_LENGTH\n",
        "\n",
        "        data = []\n",
        "        conversations = []\n",
        "        if read_one_file:\n",
        "            with open(\"./dialogs.txt\", \"r\", encoding=\"latin1\") as f:\n",
        "                raw = f.read()\n",
        "            for dialog in raw.split(\"#/\"):\n",
        "                cur_conversation = []\n",
        "                for sentence in dialog.split(\";\"):\n",
        "                    data += [sentence]\n",
        "                    cur_conversation += [sentence]\n",
        "                conversations += [(cur_conversation)]\n",
        "        else:\n",
        "            if type(self.data_dir_path) != list:\n",
        "                self.data_dir_path = [self.data_dir_path]\n",
        "            for dataset_path in self.data_dir_path:\n",
        "                for dialog in os.listdir(dataset_path):\n",
        "                        with open(f\"{dataset_path}/{dialog}\", \"r\") as f:\n",
        "                            cur_conversation = []\n",
        "                            for idx, line in enumerate(f.read().split(\"\\n\")):\n",
        "                                content = \":\".join(line.split(\":\")[1:]).strip()\n",
        "                                if len(content) > 0:\n",
        "                                    if dataset_path == \"./data_stories\" and idx == 0:\n",
        "                                        start = np.random.choice(start_sentences)\n",
        "                                        data += [f\"{start} {content}\"]\n",
        "                                        cur_conversation += [f\"{start} {content}\"]\n",
        "                                    else:\n",
        "                                        data += [content]\n",
        "                                        cur_conversation += [content]\n",
        "                        conversations += [(cur_conversation)]\n",
        "            if should_save_as_one_file:\n",
        "                save_data = \"\"\n",
        "                for idx_1, dialog in enumerate(conversations):\n",
        "                    if idx_1 > 0:\n",
        "                        save_data += \"#/\"\n",
        "\n",
        "                    for idx_2, elem in enumerate(dialog):\n",
        "                        if idx_2 == 0:\n",
        "                            save_data += f\"{elem}\"\n",
        "                        else:\n",
        "                            save_data += f\";{elem}\"\n",
        "                    with open(\"./dialogs.txt\", \"w\") as f:\n",
        "                        f.write(save_data)\n",
        "\n",
        "        # add markers and trim\n",
        "        X = []\n",
        "        y = []\n",
        "        is_conversation_beginning = []\n",
        "        for cur_conversation in conversations:\n",
        "            for idx in range(0, len(cur_conversation)-1, 2):\n",
        "                if idx == 0:\n",
        "                    is_conversation_beginning += [1]\n",
        "                else:\n",
        "                    is_conversation_beginning += [0]\n",
        "                X += [cur_conversation[idx]]\n",
        "                y += [cur_conversation[idx+1]]\n",
        "\n",
        "        self.conversations = conversations\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.is_conversation_beginning = is_conversation_beginning\n",
        "\n",
        "        # encoded_data = self.tokenizer(self.X, truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH, padding=\"max_length\") # max_length=40, padding=\"max_length\"\n",
        "        # self.X_encoded = encoded_data['input_ids']\n",
        "        # self.X_attention_mask = encoded_data['attention_mask']\n",
        "\n",
        "        encoded_data = self.tokenizer(self.y, truncation=False, return_tensors=\"pt\", max_length=MAX_LENGTH, padding=\"max_length\")\n",
        "        self.y_encoded =  encoded_data['input_ids']\n",
        "        self.y_attention_mask = encoded_data['attention_mask']\n",
        "\n",
        "    def get_context(self, idx):\n",
        "        with_context = \"\"\n",
        "        cur_idx = idx\n",
        "        while cur_idx >= 0:\n",
        "            if cur_idx == idx:\n",
        "                with_context += f\"<bot>{self.y[cur_idx]}<end>\"\n",
        "                with_context = f\"{self.X[cur_idx]}{with_context}\"\n",
        "            else:\n",
        "                with_context = f\"{self.y[cur_idx]}<sep>{with_context}\"\n",
        "                with_context = f\"{self.X[cur_idx]}<sep>{with_context}\"\n",
        "            if self.is_conversation_beginning[cur_idx] == 1:\n",
        "                break\n",
        "            cur_idx -= 1\n",
        "        #with_context = f\"<start>{with_context}\"\n",
        "        return with_context\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.get_context(idx)\n",
        "        encoded_data = self.tokenizer(X, truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH, padding=\"max_length\")\n",
        "        return (encoded_data['input_ids'], encoded_data['attention_mask'])#, self.y_encoded[idx])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ONd2dOaXoUe",
        "outputId": "fb594492-615b-4a6d-c40e-ff50775b477a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
        "tokenizer.add_special_tokens({  \"pad_token\": \"<pad>\",\n",
        "                                \"eos_token\": \"<end>\",\n",
        "                                \"sep_token\": \"<sep>\"})\n",
        "tokenizer.add_tokens([\"<bot>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2qOzeAAXoUe"
      },
      "outputs": [],
      "source": [
        "data = Dialog_Data(tokenizer=tokenizer, data_dir_path=[\"./data_stories\", \"./data_calm_bot\", \"./data\"],\n",
        "                    read_one_file=True, should_save_as_one_file=True)\n",
        "data = DataLoader(data, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9Gto4PnaHK_"
      },
      "outputs": [],
      "source": [
        "# check the data\n",
        "[print(i, \"\\n\") for i in data.dataset.X[:3]];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrAiJqjcHVtq"
      },
      "outputs": [],
      "source": [
        "data.dataset.get_context(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1EZKQOxHVtr"
      },
      "outputs": [],
      "source": [
        "data.dataset.get_context(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPJTFKhnHVtr"
      },
      "outputs": [],
      "source": [
        "data.dataset.get_context(203)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9Ns1dy6adDf"
      },
      "outputs": [],
      "source": [
        "len(data.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcjijhTTHVtr"
      },
      "outputs": [],
      "source": [
        "# # Test saved dialogs in one file\n",
        "# counter = 0\n",
        "# with open(\"./dialogs.txt\", \"r\") as f:\n",
        "#     dialogs = f.read()\n",
        "# print(f\"Dialogs amount: {len(os.listdir('./data'))}\")\n",
        "# print(f\"In one file dialogs amount: {len(dialogs.split('#'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAGX1Qq8XoUe"
      },
      "outputs": [],
      "source": [
        "BATCH_AMOUNT = 0\n",
        "for X, a in data:\n",
        "    BATCH_AMOUNT += 1\n",
        "    if BATCH_AMOUNT == 5:\n",
        "        print(\"X:\")\n",
        "        print(\"Decoded:\", tokenizer.decode(X[0][0]))\n",
        "        print(\"Encoded:\", X[0])\n",
        "        print(type(X[0]))\n",
        "        print(len(X[0]))\n",
        "        print(\"AttentionMask:\\n\", a[0])\n",
        "        print(type(a[0]))\n",
        "        print(len(a[0]))\n",
        "        # print(\"Target:\\nDecoded:\", tokenizer.decode(y[0][0]))\n",
        "        # print(\"Encoded:\", y[0])\n",
        "        # print(type(y[0]))\n",
        "        # print(len(y[0]))\n",
        "\n",
        "BATCH_AMOUNT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fm_F-1MXoUe"
      },
      "source": [
        "### Load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSDnf19QXoUe"
      },
      "outputs": [],
      "source": [
        "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex5nw1oJYQpx"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F0udytjYUST"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzZefXpkzqq1"
      },
      "outputs": [],
      "source": [
        "save_in_google(\"test_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/model.pt\", map_location=device))"
      ],
      "metadata": {
        "id": "xyURJie30MSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt7xXSLpXoUf"
      },
      "source": [
        "### First test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5jeBTWOXeyW"
      },
      "outputs": [],
      "source": [
        "def inference(prompt:str, model, tokenizer, device, padding, clear_output=True):\n",
        "    model.eval()\n",
        "    prompt = f\"{prompt}<bot>\"\n",
        "    prompt = tokenizer(prompt, return_tensors=\"pt\", padding=padding)\n",
        "    X = prompt[\"input_ids\"].to(device)\n",
        "    a = prompt[\"attention_mask\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(X, attention_mask=a, pad_token_id=tokenizer.pad_token_id,\n",
        "                                                        do_sample=True, max_length=MAX_LENGTH)\n",
        "\n",
        "    if clear_output:\n",
        "        output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        output = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "\n",
        "    if type(output) == list and len(output) == 1:\n",
        "        output = output[0]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSqQU8y1XmFZ"
      },
      "outputs": [],
      "source": [
        "inference(prompt=\"Hey, I'm feeling not so good.\", model=model, tokenizer=tokenizer,\n",
        "                                                        device=device, padding=\"max_length\", clear_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wIUfCozcxi5"
      },
      "outputs": [],
      "source": [
        "inference(prompt=\"Hey, I've been feeling really down lately.\", model=model, tokenizer=tokenizer, device=device, padding=\"max_length\", clear_output=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0zGPe1pXoUf"
      },
      "source": [
        "### Fine Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDs_W4SUwA0B"
      },
      "outputs": [],
      "source": [
        "def print_time_information(start, end=None, total_seconds=None, text=\"Total training time:\", should_print=True):\n",
        "    if type(total_seconds) is type(None):\n",
        "        if type(end) is type(None):\n",
        "            end = dt.now()\n",
        "        total_seconds = abs((start-end).total_seconds())\n",
        "\n",
        "    minutes, seconds = divmod(total_seconds, 60)\n",
        "    hours, minutes = divmod(minutes, 60)\n",
        "    days, hours = divmod(hours, 24)\n",
        "    res = f\"{text}\\n    -> {int(days)} Days\\n    -> {int(hours)} Hours\\n    -> {int(minutes)} Minutes\\n    -> {int(seconds)} Seconds\"\n",
        "    if should_print:\n",
        "        print(res)\n",
        "    return res\n",
        "\n",
        "def calculate_train_duration(epoch_start, batch_amount, epochs, cur_epoch):\n",
        "    \"\"\"\n",
        "    Call this function once after the first batch every epoch.\n",
        "    \"\"\"\n",
        "    cur_epoch += 1\n",
        "    now = dt.now()\n",
        "    duration_one_batch = abs((epoch_start-now).total_seconds())\n",
        "    duration_for_one_epoch = duration_one_batch * batch_amount\n",
        "    epochs_left = (epochs - cur_epoch) + 1    # current epoch also have to run\n",
        "    predicted_training_duration = epochs_left * duration_for_one_epoch\n",
        "    res = f\"{'-'*16}\\n\"\n",
        "    text = f\"Training will need about following time for {epochs_left} epochs:\"\n",
        "    res += print_time_information(start=epoch_start, total_seconds=predicted_training_duration, text=text, should_print=False)\n",
        "    res += f\"\\n{'-'*16}\"\n",
        "    print(res)\n",
        "    return res\n",
        "\n",
        "def get_short_duration_representation(duration, should_print=False):\n",
        "    minutes, seconds = divmod(duration, 60)\n",
        "    hours, minutes = divmod(minutes, 60)\n",
        "    days, hours = divmod(hours, 24)\n",
        "    res = f\"{int(days)}D {int(hours)}H {int(minutes)}M {int(seconds)}S\"\n",
        "    if should_print:\n",
        "        print(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqc6pXHvXoUf"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "epochs = 8\n",
        "\n",
        "loss_hist = []\n",
        "steps = 0\n",
        "\n",
        "solutions = []\n",
        "solutions_cleared = []\n",
        "epoch_durations = []\n",
        "\n",
        "start = dt.now()\n",
        "log = f\"Model V6 Trainstart: {start.strftime('%d.%m.%Y - %H:%M:%S')}\"\n",
        "\n",
        "with open(\"./log.txt\", \"w\") as f:\n",
        "    f.write(f\"Log File for Training Calm Chatbot {start.strftime('%d.%m.%Y - %H:%M:%S')}\")\n",
        "\n",
        "for cur_epoch in range(0, epochs):\n",
        "    model.train()\n",
        "    epoch_start = dt.now()\n",
        "    new_epoch = True\n",
        "    for input_ids, attention_masks in data:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_masks = attention_masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(input_ids, attention_mask=attention_masks, labels=input_ids).loss\n",
        "        loss_hist += [loss.item()]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "\n",
        "        if new_epoch:\n",
        "            time_prediction = calculate_train_duration(epoch_start, BATCH_AMOUNT, epochs, cur_epoch)\n",
        "            new_epoch = False\n",
        "\n",
        "            with open(\"./log.txt\", \"a\") as f:\n",
        "                f.write(f\"\\n\\n{time_prediction}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"./model_state_V5_{cur_epoch+1}.pt\")\n",
        "    epoch_info = f'Epoch {cur_epoch+1}/{epochs}, Training Loss: {loss.item():.4f}, Steps: {steps}, Current Time:{dt.now().strftime(\"%H:%M:%S\")}'\n",
        "    print(epoch_info)\n",
        "    test_prompt = inference(prompt=\"Hey, I'm feeling not so good.\", model=model, tokenizer=tokenizer,\n",
        "                                                        device=device, padding=True, clear_output=False)\n",
        "    solutions += [test_prompt]\n",
        "    solutions_cleared += [inference(prompt=\"Hey, I'm feeling not so good.\", model=model, tokenizer=tokenizer,\n",
        "                                                        device=device, padding=True, clear_output=True)]\n",
        "\n",
        "    epoch_durations += [abs(epoch_start-dt.now())]\n",
        "\n",
        "    with open(\"./log.txt\", \"a\") as f:\n",
        "        f.write(f\"\\n\\n{epoch_info}\\nEpoch-Duration: {get_short_duration_representation(epoch_durations[-1])}\\n\\nTest-Prompt:\\n{test_prompt}\")\n",
        "        log += f\"\\n\\n{epoch_info}\\nEpoch-Duration: {get_short_duration_representation(epoch_durations[-1])}\"\n",
        "\n",
        "duration = abs((start-dt.now()).total_seconds())\n",
        "log += print_time_information(start=epoch_start, total_seconds=duration, text=\"\\n\\nTraining duration:\", should_print=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HprXXm0SHVtt"
      },
      "outputs": [],
      "source": [
        "#print_time_information(start)\n",
        "\n",
        "# plot loss\n",
        "# fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "# ax.plot(np.arange(len(loss_hist)), loss_hist[:], label='Loss')\n",
        "# ax.set_xlabel('Learning progress')\n",
        "# ax.set_ylabel('Loss (normalized mean absolute error)')\n",
        "# ax.set_title('Loss over time')\n",
        "# ax.legend()\n",
        "# ax.grid()\n",
        "\n",
        "# save step solution-predictions:\n",
        "with open(\"./result_per_epoch.txt\", \"w\") as f:\n",
        "    res = \"\"\n",
        "    for i, cur_res in enumerate(solutions, start=1):\n",
        "        res += f\"\\n{'-'*16}\\n{i:02d}. Epoch:\\n{cur_res}\"\n",
        "    f.write(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEJnpma71GZo"
      },
      "outputs": [],
      "source": [
        "with open(\"./cleared_result_per_epoch.txt\", \"w\") as f:\n",
        "    res = \"\"\n",
        "    for i, cur_res in enumerate(solutions_cleared, start=1):\n",
        "        res += f\"\\n{'-'*16}\\n{i:02d}. Epoch:\\n{cur_res}\"\n",
        "    f.write(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePOQnOZXk29I"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('./loss_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(loss_hist, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atJ7S98hkxPv"
      },
      "outputs": [],
      "source": [
        "# plot loss\n",
        "OFFSET = 0\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "ax.plot(np.arange(len(loss_hist)-OFFSET), loss_hist[OFFSET:], label='Loss')\n",
        "ax.set_xlabel('Learning progress')\n",
        "ax.set_ylabel('Loss (normalized mean absolute error)')\n",
        "ax.set_title('Loss over time')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "plt.savefig(\"/content/gdrive/My Drive/model_V6_loss.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHQkbfDfmJqv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.ticker as mticker\n",
        "\n",
        "OFFSET = 0\n",
        "loss_series = pd.Series(loss_hist[OFFSET:])\n",
        "\n",
        "# Wenden Sie das gleitende Fenster an\n",
        "window_size = 1000  # Größe des gleitenden Fensters\n",
        "loss_rolling = loss_series.rolling(window=window_size).mean()\n",
        "\n",
        "# Zeichnen Sie die ursprünglichen und geglätteten Daten\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "#ax.plot(np.arange(len(loss_hist)), loss_hist, label='Original Loss')\n",
        "ax.plot(np.arange(len(loss_series.index)), loss_rolling, label='Smoothed Loss', linewidth=2)\n",
        "ax.set_xlabel('Learning progress')\n",
        "ax.set_ylabel('Loss (normalized mean absolute error)')\n",
        "ax.set_title('Loss over time')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "# formatter = mticker.ScalarFormatter()\n",
        "# formatter.set_scientific(False)\n",
        "# ax.yaxis.set_major_formatter(formatter)\n",
        "# ax.set_yscale('log')\n",
        "\n",
        "plt.savefig(\"/content/gdrive/My Drive/model_V6_loss_smoothed.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnfKTdk5XoUf"
      },
      "source": [
        "### Save model\n",
        "\n",
        "-> Propably save the model in a extra repository/branch and provide it as python module<br>\n",
        "-> Is model very big?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8M7Q0aUXoUf"
      },
      "source": [
        "save only weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9QFdpqfXoUf"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), MODEL_WEIGHT_PATH)\n",
        "\n",
        "# loading\n",
        "# config = transformers.GPT2Config.from_pretrained(\"gpt2\")\n",
        "# config.max_length = MAX_LENGTH #config.task_specific_params['text-generation']['max_length']\n",
        "# model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "# model.load_state_dict(torch.load(MODEL_WEIGHT_PATH))\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNbmw4pDXoUf"
      },
      "source": [
        "save whole model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6gPFu3RXoUf"
      },
      "outputs": [],
      "source": [
        "torch.save(model, MODEL_PATH)\n",
        "\n",
        "# loading\n",
        "# model = torch.load(MODEL_PATH)\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgWcVcIhtQ73"
      },
      "source": [
        "save in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idmgWLSotQlC"
      },
      "outputs": [],
      "source": [
        "save_in_google(\"model_V6_12.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTOZI6EozqrB"
      },
      "outputs": [],
      "source": [
        "save_in_google(\"model_V6_loss_hist.pkl\", loss_hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_czCNlqJzqrB"
      },
      "outputs": [],
      "source": [
        "save_in_google(\"model_V6_log.txt\", log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x56vJ4Clfoci"
      },
      "source": [
        "---\n",
        "### Ressources:\n",
        "\n",
        "- https://www.toolify.ai/ai-news/finetuning-gpt2-for-conversational-chatbots-10476\n",
        "- https://huggingface.co/docs/transformers/model_doc/gpt2\n",
        "- [PyTorch kompakt](https://www.thalia.de/shop/home/artikeldetails/A1062166688)\n",
        "- https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
        "- https://github.com/itsuncheng/fine-tuning-GPT2/tree/master\n",
        "- https://www.kaggle.com/code/pinooxd/gpt2-chatbot/notebook\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}