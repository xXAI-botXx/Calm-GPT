{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune GPT-2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Questions:\n",
    "- Is it useful to add \\<bot> statement as preparing step? -> yesss\n",
    "- Should one batch, one dialog?\n",
    "- Currently the dialogs are mixed, so only question and answer is paired right now\n",
    "    - How to fix?\n",
    "    - The batches?\n",
    "- Removing Bot answers?<br>\n",
    "    From:<br>\n",
    "    '<start> Create me a unique interactive story to calm with the topic: Ocean. <bot>:Ah, the ocean... <end>',<br>\n",
    "    \"<start> Ah, the ocean. Such a ... <end>\",<br>\n",
    "    \"<start> Yes, I can feel it...'<br>\n",
    "    <br>\n",
    "    to:<br>\n",
    "    '<start> Create me a unique interactive story to calm with the topic: Ocean. <bot>:Ah, the ocean... <end>',<br>\n",
    "    \"<start> Yes, I can feel it...'<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Dialog-based Approach:**\n",
    "   - **One Batch, One Dialog:**\n",
    "     - Treat each dialog as a separate training example. This allows the model to learn the context and flow of individual conversations.\n",
    "     - Helps the model focus on capturing the nuances of each conversation independently.\n",
    "     - Useful if your storytelling involves short, distinct dialogs.\n",
    "\n",
    "   - **Inclusion of the Past:**\n",
    "     - You can include the past history within each dialog example. Concatenate the previous turns in the conversation to provide context.\n",
    "     - This helps the model understand the context and continuity of the ongoing dialog.\n",
    "     - Be mindful of the token limit, as GPT-2 has a maximum token limit, and longer sequences might get truncated.\n",
    "\n",
    "2. **Memory and Context:**\n",
    "   - GPT-2 has a limited context window due to its fixed input size. If the conversations are long, you might lose relevant information.\n",
    "   - Consider balancing the length of your input sequences to ensure the model can capture essential details.\n",
    "\n",
    "3. **Dynamic Context Window:**\n",
    "   - Instead of a fixed history length, you could use a sliding window approach.\n",
    "   - Maintain a dynamic context window that moves along the conversation, incorporating the most recent interactions.\n",
    "\n",
    "4. **Experiment and Evaluate:**\n",
    "   - It's often beneficial to experiment with different approaches to see what works best for your specific use case.\n",
    "   - Conduct thorough evaluations using validation data to ensure the model is learning effectively and providing desired responses.\n",
    "\n",
    "5. **Training Strategies:**\n",
    "   - Experiment with hyperparameters like learning rate, batch size, and the number of training epochs to fine-tune the model effectively.\n",
    "   - Monitor the model's performance on both training and validation sets.\n",
    "\n",
    "Preprocess: handling tokenization, special tokens, and managing the context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the dialogs.txt file to train the model on google colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install torch\n",
    "#!python -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./model/model.pth\"\n",
    "MODEL_WEIGHT_PATH = \"./model/model_weights.pth\"\n",
    "ONNX_PATH = \"./model/model.onnx\"\n",
    "# \".pt\", \".pth\", \".pkl\", or \".h5\"\n",
    "\n",
    "class Dialog_Data(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, data_dir_path=\"./data\", read_one_file=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_dir_path = data_dir_path\n",
    "        self.read_data(read_one_file, read_one_file)\n",
    "\n",
    "    def read_data(self, data_dir_path, read_one_file, should_save_as_one_file=True):\n",
    "        data = []\n",
    "        conversations = []\n",
    "        if read_one_file:\n",
    "            with open(\"./dialogs.txt\", \"r\") as f:\n",
    "                raw = f.read()\n",
    "            for dialog in raw.split(\"#/\"):\n",
    "                cur_conversation = []\n",
    "                for sentence in dialog.split(\";\"):\n",
    "                    data += [sentence]\n",
    "                    cur_conversation += [sentence]\n",
    "                conversations += [(cur_conversation)]\n",
    "        else:\n",
    "            for dialog in os.listdir(self.data_dir_path):\n",
    "                    with open(f\"{self.data_dir_path}/{dialog}\", \"r\") as f:\n",
    "                        cur_conversation = []\n",
    "                        for idx, line in enumerate(f.read().split(\"\\n\")):\n",
    "                            content = \":\".join(line.split(\":\")[1:]).strip()\n",
    "                            if len(content) > 0:\n",
    "                                if idx == 0:\n",
    "                                    data += [f\"Create me a unique interactive story to calm with the topic: {content}\"]\n",
    "                                    cur_conversation += [f\"Create me a unique interactive story to calm with the topic: {content}\"]\n",
    "                                else:\n",
    "                                    data += [content]\n",
    "                                    cur_conversation += [content]\n",
    "                    conversations += [(cur_conversation)]\n",
    "            if should_save_as_one_file:\n",
    "                save_data = \"\"\n",
    "                for idx_1, dialog in enumerate(conversations):\n",
    "                    if idx_1 > 0:\n",
    "                        save_data += \"#/\"\n",
    "\n",
    "                    for idx_2, elem in enumerate(dialog):\n",
    "                        if idx_2 == 0:\n",
    "                            save_data += f\"{elem}\"\n",
    "                        else:\n",
    "                            save_data += f\";{elem}\"\n",
    "                    with open(\"./dialogs.txt\", \"w\") as f:\n",
    "                        f.write(save_data)\n",
    "\n",
    "        # add markers: \n",
    "        for idx in range(0, len(data)-1):    # last elem should be skipped\n",
    "            data[idx] = f\"<start> {data[idx]} <bot>:{data[idx+1]} <end>\"\n",
    "\n",
    "        self.conversations = conversations\n",
    "        self.data = data[:-1]\n",
    "        self.encoded_data = self.tokenizer(self.data, truncation=True)\n",
    "\n",
    "        self.input_ids = self.encoded_data['input_ids']\n",
    "        self.attention_mask = self.encoded_data['attention_mask']\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.attention_mask[idx])\n",
    "        # conversation = self.conversations[idx]\n",
    "        # inputs = self.tokenizer.encode(conversation, max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # return {\n",
    "        #     \"input_ids\": inputs.flatten(),\n",
    "        # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({  \"pad_token\": \"<pad>\",\n",
    "                                \"bos_token\": \"<start>\",\n",
    "                                \"eos_token\": \"<end>\"})\n",
    "tokenizer.add_tokens([\"<bot>:\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dialog_Data(tokenizer=tokenizer, read_one_file=False)\n",
    "data = DataLoader(data, batch_size=1)\n",
    "#  make the batch-size bigger, 62, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogs amount: 3617\n",
      "In one file dialogs amount: 3617\n"
     ]
    }
   ],
   "source": [
    "# Test saved dialogs in one file\n",
    "if True == False:\n",
    "    counter = 0\n",
    "    with open(\"./dialogs.txt\", \"r\") as f:\n",
    "        dialogs = f.read()\n",
    "    print(f\"Dialogs amount: {len(os.listdir('./data'))}\")\n",
    "    print(f\"In one file dialogs amount: {len(dialogs.split('#'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([50258]), tensor([13610]), tensor([502]), tensor([257]), tensor([3748]), tensor([14333]), tensor([1621]), tensor([284]), tensor([9480]), tensor([351]), tensor([262]), tensor([7243]), tensor([25]), tensor([10692]), tensor([13]), tensor([220]), tensor([50260]), tensor([10910]), tensor([11]), tensor([262]), tensor([9151]), tensor([13]), tensor([8013]), tensor([257]), tensor([5909]), tensor([11]), tensor([384]), tensor([25924]), tensor([1295]), tensor([13]), tensor([13872]), tensor([534]), tensor([2951]), tensor([329]), tensor([257]), tensor([2589]), tensor([290]), tensor([1011]), tensor([257]), tensor([2769]), tensor([8033]), tensor([11]), tensor([34140]), tensor([262]), tensor([36021]), tensor([21212]), tensor([286]), tensor([262]), tensor([5417]), tensor([13]), tensor([2735]), tensor([11]), tensor([4286]), tensor([3511]), tensor([5055]), tensor([319]), tensor([257]), tensor([4950]), tensor([44039]), tensor([10481]), tensor([11]), tensor([262]), tensor([23125]), tensor([286]), tensor([262]), tensor([10861]), tensor([4252]), tensor([15241]), tensor([534]), tensor([4168]), tensor([13]), tensor([1680]), tensor([345]), tensor([1254]), tensor([340]), tensor([30]), tensor([220]), tensor([50259])]\n",
      "[tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28912"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, a in data:\n",
    "    counter += 1\n",
    "    if counter < 2:\n",
    "        print(i)\n",
    "        print(a)\n",
    "    \n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50261, 768)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.GPT2Config.from_pretrained(\"gpt2\")\n",
    "config.do_sample = config.task_specific_params['text-generation']['do_sample']\n",
    "config.max_length = config.task_specific_params['text-generation']['max_length']\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Create me an interactive story to calm me down.\\n\\nI'm an experienced writer and I've been writing for a long time and it's something to take a deep breath in. You always say you can't write your own dialogue without having done\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Create me an interactive story to calm me down.\"\n",
    "encoded = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(f\"Encoding: {encoded}\\n\")\n",
    "res = tokenizer.decode(model.generate(encoded)[0])\n",
    "print(f\"Prompt:\\n{prompt}\\n\\nResult:\\n{res}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torcg.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "epochs=10\n",
    "\n",
    "loss_hist = []\n",
    "steps = 0\n",
    "\n",
    "for cur_epoch in range(0, epochs):\n",
    "    model.train()\n",
    "    for input_ids, attention_mask in data:\n",
    "        # input_ids = input_ids.to(device)\n",
    "        # attention_mask = attention_mask.to(device)\n",
    "        # shift labels?\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids, attention_maks=attention_mask, labels=input_ids).loss\n",
    "        loss_hist += [loss.item()]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "    torch.save(model.state_dict(), \"./model_state.pt\")\n",
    "    print(f'Epoch {cur_epoch+1}/{epochs}, Training Loss: {loss.item():.4f}, Steps: {steps}')\n",
    "\n",
    "# plot loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "ax.plot(np.arange(len(loss_hist)), loss_hist, label='Loss')\n",
    "ax.set_xlabel('Learning progress')\n",
    "ax.set_ylabel('Loss (normalized mean absolute error)')\n",
    "ax.set_title('Loss over time')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model\n",
    "\n",
    "-> Propably save the model in a extra repository/branch and provide it as python module<br>\n",
    "-> Is model very big?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save only weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_WEIGHT_PATH)\n",
    "\n",
    "# loading\n",
    "# config = transformers.GPT2Config.from_pretrained(\"gpt2\")\n",
    "# config.do_sample = config.task_specific_params['text-generation']['do_sample']\n",
    "# config.max_length = config.task_specific_params['text-generation']['max_length']\n",
    "# model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# model.load_state_dict(torch.load(MODEL_WEIGHT_PATH))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_PATH)\n",
    "\n",
    "# loading\n",
    "# model = torch.load(MODEL_PATH)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save as ONNX\n",
    "\n",
    "see here -> https://onnxruntime.ai/docs/get-started/with-python.html<br>\n",
    "or here -> https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_data():\n",
    "    for i, a in data:\n",
    "        return i, a\n",
    "i, a = get_example_data()\n",
    "\n",
    "# or:\n",
    "\n",
    "# text = \"Text from the news article\"\n",
    "# text = torch.tensor(text_pipeline(text))\n",
    "# offsets = torch.tensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,                     # model being run\n",
    "                  (i, a),                    # model input (or a tuple for multiple inputs)\n",
    "                  ONNX_PATH,                 # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output']  # the model's output names\n",
    "                    )\n",
    "\n",
    "# loading\n",
    "# import onnx\n",
    "\n",
    "# onnx_model = onnx.load(ONNX_PATH)\n",
    "# onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# import onnxruntime as ort\n",
    "# import numpy as np\n",
    "# ort_sess = ort.InferenceSession('ag_news_model.onnx')\n",
    "# outputs = ort_sess.run(None, {'input': text.numpy(),\n",
    "#                             'offsets':  torch.tensor([0]).numpy()})\n",
    "# # Print Result\n",
    "# result = outputs[0].argmax(axis=1)+1\n",
    "# print(\"This is a %s news\" %ag_news_label[result[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt:str, tokenizer):\n",
    "    prompt = f\"<startofstring> {data[idx]} <bot>:\"\n",
    "    prompt = tokenizer(prompt)\n",
    "    output = model.generate(prompt)\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(inp):\n",
    "  inp = \" \" + inp + \" : \"\n",
    "  inp = tokenizer(inp, return_tensors=\"pt\")\n",
    "  X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
    "  a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
    "\n",
    "  output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
    "\n",
    "  output = tokenizer.decode(output[0])\n",
    "\n",
    "  return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
