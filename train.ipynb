{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jWfzNlZXoUa"
      },
      "source": [
        "# Fine Tune GPT-2 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzL2cimNXoUb"
      },
      "source": [
        "Open Questions:\n",
        "- Is it useful to add \\<bot> statement as preparing step? -> yesss\n",
        "- Should one batch, one dialog?\n",
        "- Currently the dialogs are mixed, so only question and answer is paired right now\n",
        "    - How to fix?\n",
        "    - The batches?\n",
        "- Removing Bot answers?<br>\n",
        "    From:<br>\n",
        "    '<start> Create me a unique interactive story to calm with the topic: Ocean. <bot>:Ah, the ocean... <end>',<br>\n",
        "    \"<start> Ah, the ocean. Such a ... <end>\",<br>\n",
        "    \"<start> Yes, I can feel it...'<br>\n",
        "    <br>\n",
        "    to:<br>\n",
        "    '<start> Create me a unique interactive story to calm with the topic: Ocean. <bot>:Ah, the ocean... <end>',<br>\n",
        "    \"<start> Yes, I can feel it...'<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxvjNZgAXoUc"
      },
      "source": [
        "\n",
        "1. **Dialog-based Approach:**\n",
        "   - **One Batch, One Dialog:**\n",
        "     - Treat each dialog as a separate training example. This allows the model to learn the context and flow of individual conversations.\n",
        "     - Helps the model focus on capturing the nuances of each conversation independently.\n",
        "     - Useful if your storytelling involves short, distinct dialogs.\n",
        "\n",
        "   - **Inclusion of the Past:**\n",
        "     - You can include the past history within each dialog example. Concatenate the previous turns in the conversation to provide context.\n",
        "     - This helps the model understand the context and continuity of the ongoing dialog.\n",
        "     - Be mindful of the token limit, as GPT-2 has a maximum token limit, and longer sequences might get truncated.\n",
        "\n",
        "2. **Memory and Context:**\n",
        "   - GPT-2 has a limited context window due to its fixed input size. If the conversations are long, you might lose relevant information.\n",
        "   - Consider balancing the length of your input sequences to ensure the model can capture essential details.\n",
        "\n",
        "3. **Dynamic Context Window:**\n",
        "   - Instead of a fixed history length, you could use a sliding window approach.\n",
        "   - Maintain a dynamic context window that moves along the conversation, incorporating the most recent interactions.\n",
        "\n",
        "4. **Experiment and Evaluate:**\n",
        "   - It's often beneficial to experiment with different approaches to see what works best for your specific use case.\n",
        "   - Conduct thorough evaluations using validation data to ensure the model is learning effectively and providing desired responses.\n",
        "\n",
        "5. **Training Strategies:**\n",
        "   - Experiment with hyperparameters like learning rate, batch size, and the number of training epochs to fine-tune the model effectively.\n",
        "   - Monitor the model's performance on both training and validation sets.\n",
        "\n",
        "Preprocess: handling tokenization, special tokens, and managing the context window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi2RT8J7XoUc"
      },
      "source": [
        "Hint: Use the dialogs.txt file to train the model on google colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhd3wPF2XoUc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jOu4dG9sXoUc",
        "outputId": "914505d6-83ea-41c4-c99d-33fd29b62a10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "697uWwk6XoUd"
      },
      "outputs": [],
      "source": [
        "#!python -m pip install torch\n",
        "#!python -m pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MrZaWIOKXoUd"
      },
      "outputs": [],
      "source": [
        "#from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "import json\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbGx0tsncS8S",
        "outputId": "5b823b30-f269-4863-f1f3-3b72e087fcf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'dialogs.txt', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "os.listdir(\"./\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgi402IpXoUd"
      },
      "source": [
        "### Load and Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VFiQ05wnXoUd"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"./model/model.pth\"\n",
        "MODEL_WEIGHT_PATH = \"./model/model_weights.pth\"\n",
        "ONNX_PATH = \"./model/model.onnx\"\n",
        "MAX_LENGTH = 100\n",
        "# \".pt\", \".pth\", \".pkl\", or \".h5\"\n",
        "\n",
        "class Dialog_Data(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, data_dir_path=\"./data\", read_one_file=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data_dir_path = data_dir_path\n",
        "        self.read_data(data_dir_path, read_one_file)\n",
        "\n",
        "    def read_data(self, data_dir_path, read_one_file, should_save_as_one_file=True):\n",
        "        data = []\n",
        "        conversations = []\n",
        "        if read_one_file:\n",
        "            with open(\"./dialogs.txt\", \"r\", encoding=\"latin1\") as f:\n",
        "                raw = f.read()\n",
        "            for dialog in raw.split(\"#/\"):\n",
        "                cur_conversation = []\n",
        "                for sentence in dialog.split(\";\"):\n",
        "                    data += [sentence]\n",
        "                    cur_conversation += [sentence]\n",
        "                conversations += [(cur_conversation)]\n",
        "        else:\n",
        "            for dialog in os.listdir(self.data_dir_path):\n",
        "                    with open(f\"{self.data_dir_path}/{dialog}\", \"r\") as f:\n",
        "                        cur_conversation = []\n",
        "                        for idx, line in enumerate(f.read().split(\"\\n\")):\n",
        "                            content = \":\".join(line.split(\":\")[1:]).strip()\n",
        "                            if len(content) > 0:\n",
        "                                if idx == 0:\n",
        "                                    data += [f\"Create me a unique interactive story to calm with the topic: {content}\"]\n",
        "                                    cur_conversation += [f\"Create me a unique interactive story to calm with the topic: {content}\"]\n",
        "                                else:\n",
        "                                    data += [content]\n",
        "                                    cur_conversation += [content]\n",
        "                    conversations += [(cur_conversation)]\n",
        "            if should_save_as_one_file:\n",
        "                save_data = \"\"\n",
        "                for idx_1, dialog in enumerate(conversations):\n",
        "                    if idx_1 > 0:\n",
        "                        save_data += \"#/\"\n",
        "\n",
        "                    for idx_2, elem in enumerate(dialog):\n",
        "                        if idx_2 == 0:\n",
        "                            save_data += f\"{elem}\"\n",
        "                        else:\n",
        "                            save_data += f\";{elem}\"\n",
        "                    with open(\"./dialogs.txt\", \"w\") as f:\n",
        "                        f.write(save_data)\n",
        "\n",
        "        # add markers:\n",
        "        for idx in range(0, len(data)-1):    # last elem should be skipped\n",
        "                data[idx] = f\"<start> {data[idx]} <bot>:{data[idx+1]} <end>\"\n",
        "\n",
        "        self.conversations = conversations\n",
        "        self.data = data[:-1:2]    # [:3000]\n",
        "        self.encoded_data = self.tokenizer(self.data, truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH, padding=\"max_length\") # max_length=40, padding=\"max_length\"\n",
        "\n",
        "        self.input_ids = self.encoded_data['input_ids']\n",
        "        self.attention_mask = self.encoded_data['attention_mask']\n",
        "\n",
        "        # set max len of the input and mask -> for padding the data\n",
        "        # MAX_INPUT_LEN = 0\n",
        "        # for cur_i in self.input_ids:\n",
        "        #     if len(cur_i) > MAX_INPUT_LEN:\n",
        "        #         MAX_INPUT_LEN = len(cur_i)\n",
        "        # self.MAX_INPUT_LEN = MAX_INPUT_LEN\n",
        "\n",
        "        # MAX_ATTENTION_LEN = 0\n",
        "        # for cur_i in self.attention_mask:\n",
        "        #     if len(cur_i) > MAX_ATTENTION_LEN:\n",
        "        #         MAX_ATTENTION_LEN = len(cur_i)\n",
        "        # self.MAX_ATTENTION_LEN = MAX_ATTENTION_LEN\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # transform to tensor\n",
        "        # input_ = torch.Tensor(self.input_ids[idx])\n",
        "        # attention_mask_ = torch.Tensor(self.attention_mask[idx])\n",
        "\n",
        "        # add padding, that every batch have the same size\n",
        "        # padding_input = self.MAX_INPUT_LEN - len(input_)\n",
        "        # padding_attention_mask = self.MAX_ATTENTION_LEN - len(attention_mask_)\n",
        "        # if padding_input > 0:\n",
        "        #     input_ = torch.cat((input_, torch.zeros(padding_input)))\n",
        "        # if padding_attention_mask > 0:\n",
        "        #     attention_mask_ = torch.cat((attention_mask_, torch.zeros(padding_attention_mask)))\n",
        "        return (self.input_ids[idx], self.attention_mask[idx])\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ONd2dOaXoUe",
        "outputId": "4b599301-73e7-4139-c385-5ac72bded5b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({  \"pad_token\": \"<pad>\",\n",
        "                                \"bos_token\": \"<start>\",\n",
        "                                \"eos_token\": \"<end>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r2qOzeAAXoUe"
      },
      "outputs": [],
      "source": [
        "data = Dialog_Data(tokenizer=tokenizer, read_one_file=True)\n",
        "data = DataLoader(data, batch_size=64)\n",
        "#  make the batch-size bigger, 64, 128"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data\n",
        "[print(i, \"\\n\") for i in data.dataset.data[:3]];"
      ],
      "metadata": {
        "id": "r9Gto4PnaHK_",
        "outputId": "331ae506-d101-40e7-dd9f-dda9835e323d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> Create me a unique interactive story to calm with the topic: Ocean. <bot>:Ah, the ocean. Such a vast, serene place. Close your eyes for a moment and take a deep breath, imagining the salty scent of the sea. Now, picture yourself standing on a beautiful sandy beach, the warmth of the golden sun touching your skin. Can you feel it? <end> \n",
            "\n",
            "<start> Yes, I can feel it. It's so inviting and peaceful. <bot>:As you stand there, listen closely. Can you hear the gentle rhythm of the waves washing ashore? It's as if the ocean is whispering a calming melody just for you. Take a moment to tune in and let these soothing sounds wash away any stress or worries you may have. <end> \n",
            "\n",
            "<start> I am focusing on the sounds of the waves, feeling them melting away my tension. It's like a lullaby for the soul. <bot>:Now, let your imagination guide you towards the water's edge. As you approach, notice how the sand feels beneath your feet - soft and welcoming, with each grain embracing your toes. With each step, feel the water softly brush against your feet, refreshing and revitalizing. <end> \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data.dataset.data)"
      ],
      "metadata": {
        "id": "Z9Ns1dy6adDf",
        "outputId": "ca51ef37-681d-4bfb-ef5c-5b04fb2ff226",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29997"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RPbFzFnhXoUe"
      },
      "outputs": [],
      "source": [
        "# # Test saved dialogs in one file\n",
        "# counter = 0\n",
        "# with open(\"./dialogs.txt\", \"r\") as f:\n",
        "#     dialogs = f.read()\n",
        "# print(f\"Dialogs amount: {len(os.listdir('./data'))}\")\n",
        "# print(f\"In one file dialogs amount: {len(dialogs.split('#'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAGX1Qq8XoUe",
        "outputId": "fc60e9c0-0007-4691-e5d6-4d1c8d1ea0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50258, 13610,   502,  ..., 50257, 50257, 50257],\n",
            "        [50258,  3363,    11,  ..., 50257, 50257, 50257],\n",
            "        [50258,   314,   716,  ..., 50257, 50257, 50257],\n",
            "        ...,\n",
            "        [50258,  1081,   345,  ..., 50257, 50257, 50257],\n",
            "        [50258,  2295,  1671,  ..., 50257, 50257, 50257],\n",
            "        [50258,  1081,   262,  ..., 50257, 50257, 50257]])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "469"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "counter = 0\n",
        "for i, a in data:\n",
        "    counter += 1\n",
        "    if counter <= 1:\n",
        "        print(i)\n",
        "        print(type(i))\n",
        "        print(a)\n",
        "        print(type(a))\n",
        "\n",
        "counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fm_F-1MXoUe"
      },
      "source": [
        "### Load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSDnf19QXoUe",
        "outputId": "1b3b8846-718b-4d67-c3c3-d1c751243e63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50261, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50261, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "config = transformers.GPT2Config.from_pretrained(\"gpt2\")\n",
        "config.do_sample = config.task_specific_params['text-generation']['do_sample']\n",
        "config.max_length = config.task_specific_params['text-generation']['max_length']\n",
        "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "ex5nw1oJYQpx",
        "outputId": "d717b873-d17a-4f93-90fe-d7d11668e806",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "3F0udytjYUST"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt7xXSLpXoUf"
      },
      "source": [
        "### First test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uQvjlRZXXoUf"
      },
      "outputs": [],
      "source": [
        "# prompt = \"Create me an interactive story to calm me down.\"\n",
        "# encoded = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "# print(f\"Encoding: {encoded}\\n\")\n",
        "# res = tokenizer.decode(model.generate(encoded)[0])\n",
        "# print(f\"Prompt:\\n{prompt}\\n\\nResult:\\n{res}\");"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(prompt:str, model, tokenizer, device, padding, clear_output=True):\n",
        "    model.eval()\n",
        "\n",
        "    prompt = f\"<startofstring> {prompt} <bot>:\"\n",
        "    prompt = tokenizer(prompt, return_tensors=\"pt\", padding=padding)\n",
        "    X = prompt[\"input_ids\"].to(device)\n",
        "    a = prompt[\"attention_mask\"].to(device)\n",
        "    output = model.generate(X, attention_mask=a, pad_token_id=tokenizer.eos_token_id)\n",
        "    output = tokenizer.decode(output[0])\n",
        "\n",
        "    # clean output:\n",
        "    if clear_output:\n",
        "        if \"<bot>:\" in output:\n",
        "            output = output.split(\"<bot>:\")[1:]\n",
        "\n",
        "        if \"<endofstring>\" in output:\n",
        "            output = output.replace(\"<endofstring>\", \"\")\n",
        "\n",
        "    if type(output) == list and len(output) == 1:\n",
        "        output = output[0]\n",
        "    return output"
      ],
      "metadata": {
        "id": "z5jeBTWOXeyW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(prompt=\"Create me a unique interactive story to calm with the topic: Ocean.\", model=model, tokenizer=tokenizer,\n",
        "                                                        device=device, padding=\"max_length\", clear_output=True)"
      ],
      "metadata": {
        "id": "KSqQU8y1XmFZ",
        "outputId": "64fb6f04-0ddb-4d08-8639-8fec24d2f889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 1024, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference(prompt=\"Hey.\", model=model, tokenizer=tokenizer, device=device, padding=\"max_length\", clear_output=False)"
      ],
      "metadata": {
        "id": "1wIUfCozcxi5",
        "outputId": "c05d33c0-6931-4d88-e3be-5f3b086ceb94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 1024, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<startofstring> Hey. <bot>:<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0zGPe1pXoUf"
      },
      "source": [
        "### Fine Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqc6pXHvXoUf"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "epochs = 12\n",
        "\n",
        "loss_hist = []\n",
        "steps = 0\n",
        "\n",
        "solutions = []\n",
        "solutions_cleared = []\n",
        "\n",
        "for cur_epoch in range(0, epochs):\n",
        "    model.train()\n",
        "    for input_ids, attention_mask in data:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        # shift labels?\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(input_ids, attention_mask=attention_mask, labels=input_ids).loss\n",
        "        loss_hist += [loss.item()]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "    torch.save(model.state_dict(), \"./model_state.pt\")\n",
        "    print(f'Epoch {cur_epoch+1}/{epochs}, Training Loss: {loss.item():.4f}, Steps: {steps}')\n",
        "    solutions += [inference(prompt=\"Create me a unique interactive story to calm with the topic: Ocean.\", model=model, tokenizer=tokenizer,\n",
        "                                                        device=device, padding=True, clear_output=False)]\n",
        "    solutions_cleared += [inference(prompt=\"Create me a unique interactive story to calm with the topic: Ocean.\", model=model, tokenizer=tokenizer,\n",
        "                                                        device=device, padding=True, clear_output=True)]\n",
        "\n",
        "# plot loss\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "ax.plot(np.arange(len(loss_hist)), loss_hist, label='Loss')\n",
        "ax.set_xlabel('Learning progress')\n",
        "ax.set_ylabel('Loss (normalized mean absolute error)')\n",
        "ax.set_title('Loss over time')\n",
        "ax.legend()\n",
        "ax.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnfKTdk5XoUf"
      },
      "source": [
        "### Save model\n",
        "\n",
        "-> Propably save the model in a extra repository/branch and provide it as python module<br>\n",
        "-> Is model very big?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8M7Q0aUXoUf"
      },
      "source": [
        "save only weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9QFdpqfXoUf"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), MODEL_WEIGHT_PATH)\n",
        "\n",
        "# loading\n",
        "# config = transformers.GPT2Config.from_pretrained(\"gpt2\")\n",
        "# config.do_sample = config.task_specific_params['text-generation']['do_sample']\n",
        "# config.max_length = config.task_specific_params['text-generation']['max_length']\n",
        "# model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# model.load_state_dict(torch.load(MODEL_WEIGHT_PATH))\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNbmw4pDXoUf"
      },
      "source": [
        "save whole model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6gPFu3RXoUf"
      },
      "outputs": [],
      "source": [
        "torch.save(model, MODEL_PATH)\n",
        "\n",
        "# loading\n",
        "# model = torch.load(MODEL_PATH)\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrC-VrsWXoUf"
      },
      "source": [
        "save as ONNX\n",
        "\n",
        "see here -> https://onnxruntime.ai/docs/get-started/with-python.html<br>\n",
        "or here -> https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP4pPCQvXoUg"
      },
      "outputs": [],
      "source": [
        "def get_example_data():\n",
        "    for i, a in data:\n",
        "        return i, a\n",
        "i, a = get_example_data()\n",
        "\n",
        "# or:\n",
        "\n",
        "# text = \"Text from the news article\"\n",
        "# text = torch.tensor(text_pipeline(text))\n",
        "# offsets = torch.tensor([0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH37ZEvaXoUg"
      },
      "outputs": [],
      "source": [
        "torch.onnx.export(model,                     # model being run\n",
        "                  (i, a),                    # model input (or a tuple for multiple inputs)\n",
        "                  ONNX_PATH,                 # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=10,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output']  # the model's output names\n",
        "                    )\n",
        "\n",
        "# loading\n",
        "# import onnx\n",
        "\n",
        "# onnx_model = onnx.load(ONNX_PATH)\n",
        "# onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# import onnxruntime as ort\n",
        "# import numpy as np\n",
        "# ort_sess = ort.InferenceSession('ag_news_model.onnx')\n",
        "# outputs = ort_sess.run(None, {'input': text.numpy(),\n",
        "#                             'offsets':  torch.tensor([0]).numpy()})\n",
        "# # Print Result\n",
        "# result = outputs[0].argmax(axis=1)+1\n",
        "# print(\"This is a %s news\" %ag_news_label[result[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PDOp4CVXoUg"
      },
      "source": [
        "### Use the model\n",
        "\n",
        "-> Test the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input()\n",
        "    if user_input.lower() in [\"q\", \"quit\", \"e\", \"exit\", \"\", \"x\"]:\n",
        "        print(\"See you later! I hope you had fun ^^\")\n",
        "        break\n",
        "    print(\"Bot:\", inference(user_input, model=model, device=device, tokenizer=tokenizer, padding=\"max_length\", clear_output=False))"
      ],
      "metadata": {
        "id": "-YmHF_1UU1jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mX9kJzSXoUg"
      },
      "outputs": [],
      "source": [
        "# def infer(inp):\n",
        "#   inp = \" \" + inp + \" : \"\n",
        "#   inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "#   X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
        "#   a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
        "\n",
        "#   output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
        "\n",
        "#   output = tokenizer.decode(output[0])\n",
        "\n",
        "#   return output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}